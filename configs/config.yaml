# Default configuration for LLM OptiBench
# Override model by using: python main.py --model tinyllama|phi2|mistral

model:
  name: "TinyLlama/TinyLlama-1.1B-Chat-v1.0" # Default
  device: "cuda"
  max_length: 512

experiment:
  dataset_name: "squad_v2"
  batch_size: 2
  dataset_subset_size: 100
  seed: 42

optimization:
  # Quantization settings
  quantization_method: "awq" # "bitsandbytes" or "awq"
  awq_calibration_samples: 128

  load_in_4bit: true
  bnb_4bit_quant_type: "nf4"
  bnb_4bit_compute_dtype: "float16"

  # Pruning settings
  pruning_amount: 0.2
  pruning_method: "wanda"
  wanda_calibration_samples: 128

  # Fine-tuning after pruning
  finetune_after_pruning: false
  lora_r: 8
  lora_alpha: 16
  lora_dropout: 0.05
  finetune_epochs: 1
  finetune_learning_rate: 2e-4
  finetune_batch_size: 2

paths:
  output_dir: "results"
  data_dir: "data"
  export_models: false
  export_dir: "exports"

tracking:
  use_mlflow: true
  experiment_name: "LLM_OptiBench"
  tracking_uri: "./mlruns"
