{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM OptiBench: MLflow Analysis\n",
    "\n",
    "This notebook connects to the local MLflow tracking server to analyze and visualize the trade-offs between different LLM optimization techniques (Baseline, Quantization, Pruning)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to MLflow tracking URI: ../mlruns\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as seaborn\n",
    "\n",
    "# Set aesthetic style for plots\n",
    "plt.style.use('ggplot')\n",
    "seaborn.set_theme(style=\"whitegrid\")\n",
    "\n",
    "# Connect to MLflow tracking URI\n",
    "mlflow.set_tracking_uri(\"../mlruns\")\n",
    "\n",
    "print(f\"Connected to MLflow tracking URI: {mlflow.get_tracking_uri()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Fetch Experiment Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment 'LLM_OptiBench' not found. Please run the main.py pipeline first.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gust/Documentos/git_codes/llm-optibench/.venv/lib/python3.11/site-packages/mlflow/tracking/_tracking_service/utils.py:184: FutureWarning: The filesystem tracking backend (e.g., './mlruns') is deprecated as of February 2026. Consider transitioning to a database backend (e.g., 'sqlite:///mlflow.db') to take advantage of the latest MLflow features. See https://mlflow.org/docs/latest/self-hosting/migrate-from-file-store for migration guidance.\n",
      "  return FileStore(store_uri, store_uri)\n"
     ]
    }
   ],
   "source": [
    "experiment_name = \"LLM_OptiBench\"\n",
    "experiment = mlflow.get_experiment_by_name(experiment_name)\n",
    "\n",
    "if experiment is None:\n",
    "    print(f\"Experiment '{experiment_name}' not found. Please run the main.py pipeline first.\")\n",
    "else:\n",
    "    # Fetch all nested runs (where metrics are logged)\n",
    "    runs = mlflow.search_runs(experiment_ids=[experiment.experiment_id])\n",
    "    \n",
    "    # Filter out parent runs\n",
    "    df = runs[runs[\"tags.mlflow.parentRunId\"].notna() | runs[\"tags.mlflow.runName\"].isin([\"Baseline_FP16\", \"Quantized_4bit_NF4\", \"Pruned_Unstructured\"])].copy()\n",
    "    \n",
    "    # Clean column names for easier access\n",
    "    df = df.rename(columns={\n",
    "        \"tags.mlflow.runName\": \"Method\",\n",
    "        \"metrics.f1_score\": \"F1 Score\",\n",
    "        \"metrics.exact_match\": \"Exact Match\",\n",
    "        \"metrics.avg_latency\": \"Latency (tok/s)\",\n",
    "        \"metrics.peak_vram_gb\": \"Peak VRAM (GB)\",\n",
    "        \"metrics.model_size_gb\": \"Model Size (GB)\",\n",
    "        \"tags.mlflow.parentRunId\": \"ParentRun\"\n",
    "    })\n",
    "    \n",
    "    display(df[[\"Method\", \"F1 Score\", \"Exact Match\", \"Latency (tok/s)\", \"Peak VRAM (GB)\", \"Model Size (GB)\"]].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model Performance (F1 & Exact Match)\n",
    "Comparing the baseline model against the optimized versions to see how much accuracy is lost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'df' in locals() and not df.empty:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    # Melting for grouped bar chart\n",
    "    melted_df = df.melt(id_vars=[\"Method\"], value_vars=[\"F1 Score\", \"Exact Match\"], \n",
    "                        var_name=\"Metric\", value_name=\"Score\")\n",
    "    \n",
    "    ax = seaborn.barplot(data=melted_df, x=\"Method\", y=\"Score\", hue=\"Metric\", palette=\"viridis\")\n",
    "    plt.title(\"Accuracy Metrics by Optimization Method\")\n",
    "    plt.ylabel(\"Score (Percentage)\")\n",
    "    plt.xlabel(\"\")\n",
    "    plt.xticks(rotation=45)\n",
    "    \n",
    "    for i in ax.containers:\n",
    "        ax.bar_label(i, fmt='%.1f', padding=3)\n",
    "        \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Resource Usage (Peak VRAM & Latency)\n",
    "Visualizing the computational benefits of each technique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'df' in locals() and not df.empty:\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    # VRAM Chart\n",
    "    seaborn.barplot(data=df, x=\"Method\", y=\"Peak VRAM (GB)\", ax=ax1, palette=\"magma\")\n",
    "    ax1.set_title(\"Peak VRAM Usage (Lower is Better)\")\n",
    "    ax1.set_xticklabels(ax1.get_xticklabels(), rotation=45)\n",
    "    for i in ax1.containers:\n",
    "        ax1.bar_label(i, fmt='%.2f GB', padding=3)\n",
    "        \n",
    "    # Latency Chart\n",
    "    seaborn.barplot(data=df, x=\"Method\", y=\"Latency (tok/s)\", ax=ax2, palette=\"crest\")\n",
    "    ax2.set_title(\"Inference Latency (Higher is Better)\")\n",
    "    ax2.set_xticklabels(ax2.get_xticklabels(), rotation=45)\n",
    "    for i in ax2.containers:\n",
    "        ax2.bar_label(i, fmt='%.1f toks/s', padding=3)\n",
    "        \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Trade-off Analysis (Scatter Plot)\n",
    "The ultimate goal is finding the Pareto frontier. We want High F1, Low VRAM, and High Latency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'df' in locals() and not df.empty:\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    \n",
    "    # Scatter Plot: X=Latency, Y=F1 Score, Size=VRAM\n",
    "    scatter = seaborn.scatterplot(data=df, x=\"Latency (tok/s)\", y=\"F1 Score\", \n",
    "                                  hue=\"Method\", size=\"Peak VRAM (GB)\", sizes=(100, 1000), \n",
    "                                  alpha=0.7, palette=\"Set1\")\n",
    "    \n",
    "    plt.title(\"Trade-off: F1 Score vs Latency (Bubble Size = VRAM)\")\n",
    "    \n",
    "    # Move legend outside\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    \n",
    "    # Annotate points\n",
    "    for idx, row in df.iterrows():\n",
    "        if pd.notna(row[\"F1 Score\"]) and pd.notna(row[\"Latency (tok/s)\"]):\n",
    "            plt.annotate(row[\"Method\"], \n",
    "                         (row[\"Latency (tok/s)\"], row[\"F1 Score\"]),\n",
    "                         xytext=(10, -10), textcoords='offset points')\n",
    "            \n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
